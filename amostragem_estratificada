
#1. Análise de Cluster (Análise de Agrupamentos)
##Descrição: Utiliza algoritmos de clustering (como K-means) para agrupar indivíduos com base em características semelhantes.
##Vantagens: Identifica grupos naturais na população.
##Desvantagens: Requer software estatístico e pode ser complexo.

#Carregamento dos dados
data("dados")
df=scale(dados)
head(df, n=3)

# Número ótimo de clusters
library(factoextra)
fviz_nbclust(df, kmeans, method = "wss")+
  geom_vline(xintercept = 4, linetype = 2)

# Clusterização k-means
set.seed(123)
km.res=kmeans(df, 4, nstart=25)
print(km.res)

aggregate(dados, by=list(cluster=km.res$cluster), mean)

mtcars2=cbind(dados, cluster=km.res$cluster)
head(dados)

km.res$centers

# Vizualizando os clusters

library(ggplot2)
library(factoextra)

fviz_cluster(km.res, data=dados,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
             ellipse.type="euclid",
             star.plot=TRUE,
             repel=TRUE,
             ggtheme=theme_minimal()
             )


# 2. Análise de Componentes Principais (PCA)
##Descrição: Reduz a dimensionalidade dos dados, identificando as principais variáveis que explicam a maior parte da variância.
##Vantagens: Útil quando há muitas variáveis correlacionadas.
##Desvantagens: Pode ser difícil interpretar os componentes principais.

# Carregando os pacotes

library(tidyverse)
library(readr)
library(FactoMineR)
library(factoextra)
library(DT)
library(ggplot2)
library(ggcorrplot)
library(corrplot)
library(gt)

# Carregando os dados

dados <- read.csv("forestfires.csv", sep = ",")
datatable(dados,
           class = "row-border hover",
      options = list(
        scrollX = TRUE,
        dom = 'ltipr'
      ))

## Caso havam variáveis numéricas e não numéricas no mesmo conjunto de dados, use o comando a baixo para poder extrair as variáveis numéricas.

dados_num <- dados[,c(5:13)]
datatable(dados_num,
           class = "row-border hover",
      options = list(
        scrollX = TRUE,
        dom = 'ltipr'
      ))

# Plotando Matriz de Correlação
ggcorrplot(
            cor(dados_num),
            hc.order = T,
            type = "lower",
            lab = T,
            colors = c("#003262", "#FFFFFF", "#D21E1C")
          )

# Matriz de variância e covariância
Mcov<-cov(dados_num)
Mcov<-round(Mcov, 4)
datatable(Mcov,
           class = "row-border hover",
      options = list(
        scrollX = TRUE,
        dom = 'ltipr'
      ))


# Análise de Principais Componentes

res.pca<-PCA(dados_num, graph = F)
eig.val <- as.data.frame(get_eigenvalue(res.pca))
row.names(eig.val) <- c("Componente.1", "Componente.2", "Componente.3", "Componente.4", "Componente.5", "Componente.6", "Componente.7", "Componente.8", "Componente.9")
nomes<-rownames(eig.val)
eig.val<-cbind(nomes, eig.val)
colnames(eig.val)<-c("Componente", "Autovalor", "Percentual", "Acumulado")
gt(eig.val) %>%
  tab_header(title = "Resumo dos componentes") %>%
  fmt_number(columns = c(2:4), decimals = 4) %>%
  opt_stylize(style = 3, color = "blue") %>%
  tab_options(table.width ="500px") 

# Seguindo o princípio de Pareto (80% dos resultados são gerado por 20% das causas) deve-se selecionar a quantidade de componentes que explique 80% dos dados. NO exemplo, 5 componentes.

fviz_eig(res.pca, addlabels = TRUE, xlab='Componentes', ylab = 'Percentual da variância explicada', ylim = c(0, 50))

# Investigando cada interação entre os componentes
fviz_pca_var(res.pca, col.var = "contrib",
             axes = c(1, 2),
             gradient.cols = c("#003262", "#E7B800", "#D21E1C"),
             repel = TRUE 
)

fviz_pca_var(res.pca, col.var = "contrib",
             axes = c(3, 4),
             gradient.cols = c("#003262", "#E7B800", "#D21E1C"),
             repel = TRUE 
)

# Plotando as combinações entre todos os componentes
colors = c("#003262", "#FFFFFF", "#FF0000")
pal = colorRampPalette(colors)
grad_colors = pal(1000)
corrplot(res.pca$var$cos2, is.corr=FALSE, col=grad_colors)

# Contribuição de cada componente
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10,
             title ="Contribuição das variáveis para o componente 1"
)

fviz_contrib(res.pca, choice = "var", axes = 2, top = 10,
             title ="Contribuição das variáveis o componente 2"
)

fviz_contrib(res.pca, choice = "var", axes = 3, top = 10,
             title ="Contribuição das variáveis o componente 3"
)

# 3. Análise Discriminante
Descrição: Classifica indivíduos em grupos pré-definidos com base em variáveis preditoras.
Vantagens: Útil quando os estratos já são conhecidos.
Desvantagens: Requer conhecimento prévio dos estratos.

## Carregando pacotes
library(tidyverse)
library(MASS)
library(klaR)
set.seed(101)
sample_n(iris, 10)

## Preparando os dados
training_sample <- sample(c(TRUE, FALSE), nrow(iris), replace = T, prob = c(0.6,0.4))
train <- iris[training_sample, ]
test <- iris[!training_sample, ]

## Aplicando LDA (Análise Discriminante Linear)
lda.iris <- lda(Species ~ ., train)
lda.iris #show results

plot(lda.iris, col = as.integer(train$Species))
plot(lda.iris, dimen = 1, type = "b")

## LDA plots particionados
partimat(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=train, method="lda")

## Predições LDA
lda.train <- predict(lda.iris)
train$lda <- lda.train$class
table(train$lda,train$Species)

lda.test <- predict(lda.iris,test)
test$lda <- lda.test$class
table(test$lda,test$Species)

#Análise Discriminante Quadrática (QDA)
qda.iris <- qda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, train)
qda.iris #show results

##QDA quadráticas
partimat(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=train, method="qda")

##Predições das QDAs
qda.train <- predict(qda.iris)
train$qda <- qda.train$class
table(train$qda,train$Species)

qda.test <- predict(qda.iris,test)
test$qda <- qda.test$class
table(test$qda,test$Species)


# 4. Árvores de Decisão
Descrição: Usa algoritmos de árvore de decisão para dividir a população com base em regras de decisão.
Vantagens: Fácil de interpretar e implementar.
Desvantagens: Pode superajustar os dados.

## Particionando os dados utilizando o método holdout
library(rminer)

df = read.csv('credit_data.csv', header = T)

# O C5.0 obriga que a classe seja um factor (nem string serve)
# No caso do nosso dataset, a classe é originalmente um inteiro 0 ou 1. 
# Então convertemos para factor primeiro: 
df$default = as.factor(df$default)

# Removendo o ID do cliente, pois não será útil para os modelos.
df = df[,-1]

# Aplicando particionamento holdout, com 70% treino e 30% teste
data = holdout(df$default, ratio = 0.7, mode = 'stratified')

# O retorno da função holdout dá as posições das entradas que foram selecionadas.
# Por isso precisamos voltar ao dataframe e selecionar as linhas, conforme abaixo:
train = df[data$tr, ]
test = df[data$ts, ]

# No nosso dataframe, a classe está na quarta coluna.
X_train = train[,-4]
X_test = test[,-4]

y_train = train[,4]
y_test = test[,4]


## Treinamento e predição dos modelos

library(C50)
tree_model = C5.0(X_train, y_train)  # treinamento

### Predição
outcome = predict(tree_model, X_test, type = 'class')
outcome = cbind(test, outcome)

### Gerando a matriz de confusão
cm = table(outcome$default, outcome$outcome)
cm

## Computando as métricas manualmente:
acc1 = (cm[1,1] + cm[2,2]) / sum(cm)
prec1 = cm[2,2] / (cm[2,2] + cm[1,2] )
rec1 = cm[2,2] / (cm[2,2] + cm[2,1] )
f1sc1 = 2*(prec1*rec1/(prec1 + rec1))
c50_metrics = c(acc1,prec1,rec1,f1sc1)

### Utilizando o método rpart

library(rpart)

tree_model2 = rpart(default ~ ., data = train, method = 'class')

# A notação ~. serve para indicar que o atributo classe é "default" e todas as outras colunas
# serão usadas como atributos preditivos. Também é possível usar apenas colunas específicas 
# como atributos preditivos. Por exemplo, se quiséssemos usar apenas loan e age, seria:
# rpart(default ~ loan + age, data = train, method = 'class')

# Predição
outcome2 = predict(tree_model2, X_test, type = 'class')
outcome2 = cbind(test, outcome2)

# Gerando a matriz de confusão
cm2 = table(outcome2$default, outcome2$outcome2)
cm2

# Computando as métricas manualmente:
acc2 = (cm2[1,1] + cm2[2,2]) / sum(cm2)
prec2 = cm2[2,2] / (cm2[2,2] + cm2[1,2] )
rec2 = cm2[2,2] / (cm2[2,2] + cm2[2,1] )
f1sc2 = 2*(prec2*rec2/(prec2 + rec2))
rpart_metrics = c(acc2,prec2,rec2,f1sc2)

## Comparativo dos modelos
metrics = data.frame(cbind(c50_metrics,rpart_metrics))
rownames(metrics) = c('Accuracy','Precision','Recall','F1')
colnames(metrics) = c('C5.0','rpart')

metrics

### Visualizando as árvores
# Com o pacote C50
summary(tree_model)

# Com o pacote rport
tree_model12


## 5. Análise de Correspondência Múltipla (MCA)
#Descrição: Usada para dados categóricos, identifica padrões e agrupamentos.
#Vantagens: Adequada para variáveis categóricas.
#Desvantagens: Menos comum e pode ser complexa.

# Instalando pacotes

# install.packages("FactoMineR", "gglot2", "readxl")
# install.packages("devtools")
# devtools::install_github("kassambara/factoextra")
rm(list=ls(all=TRUE))
library("ggplot2")
library("FactoMineR")
library("factoextra")
library("readxl")
library("gplots")
library("corrplot")
library("graphics")
library("foreign")
library("readxl")
library("amap")

# ler os dados
investidor <- read_excel("perfil investidor x aplicacao x estadocivil.xlsx")
investidor 

# Codificação binária e matriz de BURT

matbinaria <- matlogic(dados)
matbinaria

dados.burt <-burt(dados)
dados.burt

# MCA with function dudi.acm
library(ade4)
# apply dudi.acm
mca1 <- dudi.acm(dados, scannf = FALSE)
# eig   eigenvalues, a vector with min(n,p) components
# c1    principal axes, data frame with p rows and nf columns
# co    column coordinates, data frame with p rows and nf columns
summary(mca1)

#coordenadas principais (BURT)
round(mca1$co, 3)

# coordenadas padrão (geradas com a matriz binária)
round(mca1$c1, 3)

# number of categories per variable
cats = apply(dados, 2, function(x) nlevels(as.factor(x)))
# data frame for ggplot
mca1_vars_df = data.frame(mca1$co, Variable = rep(names(cats), cats))
# plot
ggplot(data = mca1_vars_df, 
       aes(x = Comp1, y = Comp2, label = rownames(mca1_vars_df))) +
  geom_hline(yintercept = 0, colour = "gray70") +
  geom_vline(xintercept = 0, colour = "gray70") +
  geom_text(aes(colour = Variable)) +
  ggtitle("MCA plot of variables")


## Passos para Estabelecer Estratos:
Defina os Objetivos: Determine quais características são importantes para a estratificação.

Coleta de Dados: Colete dados sobre essas características na população.

Análise Exploratória: Use técnicas exploratórias para identificar padrões e agrupamentos.

Aplicação de Métodos Estatísticos: Aplique um dos métodos acima para definir os estratos.

Validação: Verifique se os estratos são homogêneos internamente e heterogêneos entre si.
