
#Análise de Cluster (Análise de Agrupamentos)
##Descrição: Utiliza algoritmos de clustering (como K-means) para agrupar indivíduos com base em características semelhantes.
##Vantagens: Identifica grupos naturais na população.
##Desvantagens: Requer software estatístico e pode ser complexo.

#Carregamento dos dados
data("dados")
df=scale(dados)
head(df, n=3)

# Número ótimo de clusters
library(factoextra)
fviz_nbclust(df, kmeans, method = "wss")+
  geom_vline(xintercept = 4, linetype = 2)

# Clusterização k-means
set.seed(123)
km.res=kmeans(df, 4, nstart=25)
print(km.res)

aggregate(dados, by=list(cluster=km.res$cluster), mean)

mtcars2=cbind(dados, cluster=km.res$cluster)
head(dados)

km.res$centers

# Vizualizando os clusters

library(ggplot2)
library(factoextra)

fviz_cluster(km.res, data=dados,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
             ellipse.type="euclid",
             star.plot=TRUE,
             repel=TRUE,
             ggtheme=theme_minimal()
             )


# 2. Análise de Componentes Principais (PCA)
##Descrição: Reduz a dimensionalidade dos dados, identificando as principais variáveis que explicam a maior parte da variância.
##Vantagens: Útil quando há muitas variáveis correlacionadas.
##Desvantagens: Pode ser difícil interpretar os componentes principais.

# Carregando os pacotes

library(tidyverse)
library(readr)
library(FactoMineR)
library(factoextra)
library(DT)
library(ggplot2)
library(ggcorrplot)
library(corrplot)
library(gt)

# Carregando os dados

dados <- read.csv("forestfires.csv", sep = ",")
datatable(dados,
           class = "row-border hover",
      options = list(
        scrollX = TRUE,
        dom = 'ltipr'
      ))

## Caso havam variáveis numéricas e não numéricas no mesmo conjunto de dados, use o comando a baixo para poder extrair as variáveis numéricas.

dados_num <- dados[,c(5:13)]
datatable(dados_num,
           class = "row-border hover",
      options = list(
        scrollX = TRUE,
        dom = 'ltipr'
      ))

# Plotando Matriz de Correlação
ggcorrplot(
            cor(dados_num),
            hc.order = T,
            type = "lower",
            lab = T,
            colors = c("#003262", "#FFFFFF", "#D21E1C")
          )

# Matriz de variância e covariância
Mcov<-cov(dados_num)
Mcov<-round(Mcov, 4)
datatable(Mcov,
           class = "row-border hover",
      options = list(
        scrollX = TRUE,
        dom = 'ltipr'
      ))


# Análise de Principais Componentes

res.pca<-PCA(dados_num, graph = F)
eig.val <- as.data.frame(get_eigenvalue(res.pca))
row.names(eig.val) <- c("Componente.1", "Componente.2", "Componente.3", "Componente.4", "Componente.5", "Componente.6", "Componente.7", "Componente.8", "Componente.9")
nomes<-rownames(eig.val)
eig.val<-cbind(nomes, eig.val)
colnames(eig.val)<-c("Componente", "Autovalor", "Percentual", "Acumulado")
gt(eig.val) %>%
  tab_header(title = "Resumo dos componentes") %>%
  fmt_number(columns = c(2:4), decimals = 4) %>%
  opt_stylize(style = 3, color = "blue") %>%
  tab_options(table.width ="500px") 

# Seguindo o princípio de Pareto (80% dos resultados são gerado por 20% das causas) deve-se selecionar a quantidade de componentes que explique 80% dos dados. NO exemplo, 5 componentes.

fviz_eig(res.pca, addlabels = TRUE, xlab='Componentes', ylab = 'Percentual da variância explicada', ylim = c(0, 50))

# Investigando cada interação entre os componentes
fviz_pca_var(res.pca, col.var = "contrib",
             axes = c(1, 2),
             gradient.cols = c("#003262", "#E7B800", "#D21E1C"),
             repel = TRUE 
)

fviz_pca_var(res.pca, col.var = "contrib",
             axes = c(3, 4),
             gradient.cols = c("#003262", "#E7B800", "#D21E1C"),
             repel = TRUE 
)

# Plotando as combinações entre todos os componentes
colors = c("#003262", "#FFFFFF", "#FF0000")
pal = colorRampPalette(colors)
grad_colors = pal(1000)
corrplot(res.pca$var$cos2, is.corr=FALSE, col=grad_colors)

# Contribuição de cada componente
fviz_contrib(res.pca, choice = "var", axes = 1, top = 10,
             title ="Contribuição das variáveis para o componente 1"
)

fviz_contrib(res.pca, choice = "var", axes = 2, top = 10,
             title ="Contribuição das variáveis o componente 2"
)

fviz_contrib(res.pca, choice = "var", axes = 3, top = 10,
             title ="Contribuição das variáveis o componente 3"
)

# 3. Análise Discriminante
Descrição: Classifica indivíduos em grupos pré-definidos com base em variáveis preditoras.
Vantagens: Útil quando os estratos já são conhecidos.
Desvantagens: Requer conhecimento prévio dos estratos.

## Carregando pacotes
library(tidyverse)
library(MASS)
library(klaR)
set.seed(101)
sample_n(iris, 10)

## Preparando os dados
training_sample <- sample(c(TRUE, FALSE), nrow(iris), replace = T, prob = c(0.6,0.4))
train <- iris[training_sample, ]
test <- iris[!training_sample, ]

## Aplicando LDA (Análise Discriminante Linear)
lda.iris <- lda(Species ~ ., train)
lda.iris #show results

plot(lda.iris, col = as.integer(train$Species))
plot(lda.iris, dimen = 1, type = "b")

## LDA plots particionados
partimat(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=train, method="lda")

## Predições LDA
lda.train <- predict(lda.iris)
train$lda <- lda.train$class
table(train$lda,train$Species)

lda.test <- predict(lda.iris,test)
test$lda <- lda.test$class
table(test$lda,test$Species)

#Análise Discriminante Quadrática (QDA)
qda.iris <- qda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, train)
qda.iris #show results

##QDA quadráticas
partimat(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=train, method="qda")

##Predições das QDAs
qda.train <- predict(qda.iris)
train$qda <- qda.train$class
table(train$qda,train$Species)

qda.test <- predict(qda.iris,test)
test$qda <- qda.test$class
table(test$qda,test$Species)


# 4. Árvores de Decisão
Descrição: Usa algoritmos de árvore de decisão para dividir a população com base em regras de decisão.

Vantagens: Fácil de interpretar e implementar.

Desvantagens: Pode superajustar os dados.

# 5. Análise de Correspondência Múltipla (MCA)
Descrição: Usada para dados categóricos, identifica padrões e agrupamentos.

Vantagens: Adequada para variáveis categóricas.

Desvantagens: Menos comum e pode ser complexa.

# 6. Métodos de Otimização
Descrição: Usa algoritmos de otimização para maximizar a homogeneidade dentro dos estratos e a heterogeneidade entre eles.

Vantagens: Pode ser muito eficaz.

Desvantagens: Requer conhecimento avançado de otimização.

## Passos para Estabelecer Estratos:
Defina os Objetivos: Determine quais características são importantes para a estratificação.

Coleta de Dados: Colete dados sobre essas características na população.

Análise Exploratória: Use técnicas exploratórias para identificar padrões e agrupamentos.

Aplicação de Métodos Estatísticos: Aplique um dos métodos acima para definir os estratos.

Validação: Verifique se os estratos são homogêneos internamente e heterogêneos entre si.
